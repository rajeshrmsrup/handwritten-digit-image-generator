{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZuZ-iLzGJ1DF"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import mnist\n",
        "from keras.layers import *\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, _),(_,_) = mnist.load_data()"
      ],
      "metadata": {
        "id": "EowHmlOvN-N6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be974d15-02ce-4386-8153-215ab1d1650e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "CwKDg5kkOaFK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b481737-9d45-481f-b1a7-3e072b4d896e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " X_train  = (X_train-127.5)/127.5\n",
        "\n",
        " print(X_train.min())\n",
        " print(X_train.max())"
      ],
      "metadata": {
        "id": "HGl2qYz-Ohw_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2a3e606-4c00-47ba-f540-d9a49e67229e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-1.0\n",
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TOTAL_EPOCHS = 50 #number of passes\n",
        "BATCH_SIZE = 256  #number of samples processed before the model is updated\n",
        "HALF_BATCH = 128\n",
        "\n",
        "NO_OF_BATCHES = int(X_train.shape[0]/BATCH_SIZE)\n",
        "NOISE_DIM = 100\n",
        "adam = Adam(lr = 2e-4, beta_1 = 0.5)  # lr=learning rate"
      ],
      "metadata": {
        "id": "Bv5Ylah7QqB5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df369f2b-4d8b-405d-8eaa-d8666c75aadf"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#------------------------Generator Model : UpSampling------------------------#\n",
        "generator = Sequential()\n",
        "generator.add(Dense(units = 7*7*128, input_shape = (NOISE_DIM,)))\n",
        "generator.add(Reshape((7,7,128)))\n",
        "generator.add(LeakyReLU(0.2))\n",
        "generator.add(BatchNormalization())\n",
        "\n",
        "#(7,7,128) -> (14,14,64)\n",
        "generator.add(Conv2DTranspose(64, (3,3), strides = (2, 2), padding = 'same'))\n",
        "generator.add(LeakyReLU(0.2))\n",
        "generator.add(BatchNormalization())\n",
        "\n",
        "#(14,14,64) -> (28,28,1)\n",
        "generator.add(Conv2DTranspose(1, (3,3), strides = (2, 2), padding = 'same', activation = 'tanh'))\n",
        "\n",
        "generator.compile(loss = \"binary_crossentropy\", optimizer = adam)\n",
        "#generator.summary()\n",
        "print(\"Success\")"
      ],
      "metadata": {
        "id": "90kBUGXfSB_b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fc514ea-e737-4374-f91c-c87d2b8b62a4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#------------------------Discriminator Model : DownSampling------------------------#\n",
        "#(28,28,1) -> (14,14,64)\n",
        "descriminator = Sequential()\n",
        "descriminator.add(Conv2D(64, kernel_size=(3,3), strides=(2,2), padding='same', input_shape=(28,28,1)))\n",
        "descriminator.add(LeakyReLU(0.2))\n",
        "\n",
        "#(14,14,64) -> (7,7,128)\n",
        "descriminator.add(Conv2D(128, kernel_size=(3,3), strides=(2,2), padding='same'))\n",
        "descriminator.add(LeakyReLU(0.2))\n",
        "\n",
        " #(7,7,128) -> 6272\n",
        "descriminator.add(Flatten())\n",
        "descriminator.add( Dense(100) )\n",
        "descriminator.add(LeakyReLU(0.2))\n",
        "\n",
        "descriminator.add(Dense(1, activation='sigmoid'))\n",
        "descriminator.compile(loss = \"binary_crossentropy\", optimizer=adam)\n",
        "#descriminator.summary()\n",
        "\n",
        "print(\"Success\")"
      ],
      "metadata": {
        "id": "c2LSueNBi3Gq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34d85394-cd72-411a-f86e-7734130d6994"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Combined Model\n",
        "descriminator.trainable = False\n",
        "gan_input = Input(shape = (NOISE_DIM, ))\n",
        "generated_img = generator(gan_input)\n",
        "gan_output = descriminator(generated_img)\n",
        "\n",
        "# Functional\n",
        "model = Model(gan_input, gan_output)\n",
        "model.compile(loss = \"binary_crossentropy\", optimizer=adam)\n",
        "#model.summary()\n",
        "\n",
        "print(\"Success\")"
      ],
      "metadata": {
        "id": "8X8NuRyfn1ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "655f4e42-73c4-4431-8822-4fc5879d8951"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train.reshape(-1,28,28,1)\n",
        "X_train.shape"
      ],
      "metadata": {
        "id": "7OmOeCuttqCc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25789b7b-7d4c-41b9-9616-ecd2eb7850c2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def display_images(samples = 25):\n",
        "  noise = np.random.normal(0,1,size=(samples, NOISE_DIM))\n",
        "  generated_img = generator.predict(noise)\n",
        "\n",
        "  plt.figure(figsize = (10,10))\n",
        "  \n",
        "  for i in range(samples):\n",
        "    plt.subplot(5, 5, i+1)\n",
        "    plt.imshow(generated_img[i].reshape(28,28), cmap = 'gray')\n",
        "    plt.axis(\"off\")\n",
        "  \n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "v6uzJau39lYp"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Training Loop\n",
        "d_losses = []   # Descriminator loss\n",
        "g_losses = []   # Generator loss\n",
        "\n",
        "for epoch in range(TOTAL_EPOCHS):\n",
        "  epoch_d_loss = 0.0\n",
        "  epoch_g_loss = 0.0\n",
        "\n",
        "  # Mini batch gradient decent\n",
        "  for step in range(NO_OF_BATCHES):\n",
        "    #+++++++++++++++++++++++++++++\n",
        "    #Step 1 Train Descriminator\n",
        "    descriminator.trainable = True\n",
        "\n",
        "    # get the real data\n",
        "    idx = np.random.randint(0, 60000, HALF_BATCH)\n",
        "    real_imgs = X_train[idx]\n",
        "\n",
        "    # get the fake data\n",
        "    noise = np.random.normal(0, 1, size=(HALF_BATCH, NOISE_DIM))\n",
        "    fake_imgs = generator.predict(noise)\n",
        "\n",
        "    # Labels\n",
        "    real_y = np.ones((HALF_BATCH, 1))*0.9   #One-sided label smoothing\n",
        "    fake_y = np.zeros((HALF_BATCH, 1))\n",
        "\n",
        "    # now train D\n",
        "    d_loss_real = descriminator.train_on_batch(real_imgs, real_y)\n",
        "    d_loss_fake = descriminator.train_on_batch(fake_imgs, fake_y)\n",
        "\n",
        "    d_loss = 0.5*d_loss_real + 0.5*d_loss_fake\n",
        "    epoch_d_loss += d_loss  ##########################\n",
        "\n",
        "    \n",
        "\n",
        "    #============================================\n",
        "    #step 2 Train Generator (Discriminator freeze)\n",
        "    descriminator.trainable = False\n",
        "    noise = np.random.normal(0, 1, size=(BATCH_SIZE, NOISE_DIM))\n",
        "    ground_truth_y = np.ones((BATCH_SIZE, 1))\n",
        "    g_loss = model.train_on_batch(noise, ground_truth_y)\n",
        "    epoch_g_loss += g_loss\n",
        "\n",
        "    print(\"Epoch {} Descriminator loss {}, Generator Loss {}\".format((epoch+1), epoch_d_loss/NO_OF_BATCHES, epoch_g_loss/BATCH_SIZE))\n",
        "    d_losses.append(epoch_d_loss/NO_OF_BATCHES)\n",
        "    g_losses.append(epoch_g_loss/NO_OF_BATCHES)\n",
        "\n",
        "    if(epoch+1)%10 == 0:\n",
        "      generator.save(\"generator.h5\")\n",
        "      display_images()"
      ],
      "metadata": {
        "id": "817yLKKWt4cc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}